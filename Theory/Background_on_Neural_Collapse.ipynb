{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/WilliamLockeIV/Spline-Theory-and-Neural-Collapse/blob/main/Theory/Background_on_Neural_Collapse.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_eMBKctI23JC"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2NAwg40RmvC"
      },
      "source": [
        "# Neural Collapse\n",
        "\n",
        "Neural collapse is an inductive bias observed across multiple model types and datasets, first described in Papyan et al.'s \"Prevalence of neural collapse during the terminal phase of deep learning training\" (2020) and expanded upon in subsequent papers. This notebook describes and implements the four metrics used to measure neural collapse, summarized by Hong & Ling (2024) as:\n",
        "\n",
        "\n",
        "> *   NC1 [Variability Collapse]: the feature of samples from the same class converge to a unique mean feature vector;\n",
        "> *   NC2 [Convergence to simplex ETF]: these feature vectors (after centering by their global mean) form a simplex equiangular tight frame (ETF), i.e., they share the same pairwise angles and length and have max pairwise distance;\n",
        "> *   NC3 [Self-duality]: the weight of the linear classifier converges to the corresponding feature mean (up to scalar product);\n",
        "> *   NC4 [Nearest Class Center]: the trained DNN classifies the sample by finding the closest mean feature vectors to the sample feature.\n",
        "\n",
        "(Hong & Ling, 2024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4P6wfDZ3ql8"
      },
      "source": [
        "## NC1: Variability Collapse\n",
        "\n",
        "Papyan et al. (2020) define within-class covariance as\n",
        "\n",
        "$\\sum_{W} = Ave_{i,c}\\{(\\vec{h}_{i,c}-\\vec{\\mu}_{c})(\\vec{h}_{i,c}-\\vec{\\mu}_{c})^{T}\\}$\n",
        "\n",
        "and between-class covariance as\n",
        "\n",
        "$\\sum_{B} = Ave_{c}\\{(\\vec{\\mu}_{c}-\\vec{\\mu}_{G})(\\vec{\\mu}_{c}-\\vec{\\mu}_{G})^{T}\\}$\n",
        "\n",
        "where $\\vec{h}_{i,c}$ is the feature representation of the $i^{th}$ sample of class $c$, $\\vec{\\mu}_{c}$ is the mean feature representation of class $c$, and $\\vec{\\mu}_{G}$ is the mean feature representation of the entire dataset.\n",
        "\n",
        "They then formalize NC1 as within-class covariance going to zero during the terminal phase of training (TPT), i.e.\n",
        "\n",
        "$\\sum_{W} \\to 0$\n",
        "\n",
        "They also point out that the sum of within-class covariance and between-class covariance is equal to the total covariance of the dataset:\n",
        "\n",
        "$\\sum_{W} + \\sum_{B} = \\sum_{T}$\n",
        "\n",
        "We use this last observation to check our calculation of within-class covariance and between-class covariance for both balanced and imbalanced classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0VtSVCpVR3e",
        "outputId": "0e76c62e-9416-49c3-ac3f-7d7eac3b7b73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Within-Class + Between-Class = Total Covariance\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Covariance Matrix with balanced classes\n",
        "\n",
        "# C is an m x n data matrix, where m is the number of data points and n is the number of variables / features.\n",
        "# This is transposed from the notation used in Papyan et al. (2020), but it matches my code implementation elsewhere.\n",
        "C = torch.rand((6,10))\n",
        "\n",
        "# Split the dataset C into equal-sized classes A and B\n",
        "A = C[:3]\n",
        "B = C[3:]\n",
        "\n",
        "# Calculate the global mean and class means\n",
        "C_mean = C.mean(dim=0, keepdim=True)\n",
        "A_mean = A.mean(dim=0, keepdim=True)\n",
        "B_mean = B.mean(dim=0, keepdim=True)\n",
        "class_means = torch.cat([A_mean, B_mean])\n",
        "\n",
        "# Calculate within-class covariance, between-class covariance, and total covariance\n",
        "within_class_cov = torch.stack([A.T.cov(correction=0), B.T.cov(correction=0)]).mean(dim=0)\n",
        "between_class_cov = class_means.T.cov(correction=0)\n",
        "total_cov = C.T.cov(correction=0)\n",
        "\n",
        "# Show that the sum of within-class covariance + between-class covariance = total covariance\n",
        "print('Within-Class + Between-Class = Total Covariance')\n",
        "print(torch.allclose(within_class_cov + between_class_cov, total_cov))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2V86n-0WQ2Y",
        "outputId": "e10cda3a-d057-41a7-8abd-2c4465bea243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Within-Class + Between-Class = Total Covariance\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Covariance Matrix with imbalanced classes\n",
        "\n",
        "# C = m x n data matrix\n",
        "C = torch.rand((6,10))\n",
        "\n",
        "# Split the dataset C into unequal classes A and B\n",
        "A = C[:2]\n",
        "B = C[2:]\n",
        "\n",
        "# Balance the within-class covariances by weighting each class' covariance by the number of samples in that class\n",
        "class_weights = torch.tensor([A.shape[0], B.shape[0]])\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "\n",
        "# Balance the between-class covariance by repeating each class' mean by the number of samples in that class\n",
        "C_mean = C.mean(dim=0, keepdim=True)\n",
        "A_mean = A.mean(dim=0, keepdim=True).repeat((A.shape[0],1))\n",
        "B_mean = B.mean(dim=0, keepdim=True).repeat((B.shape[0],1))\n",
        "class_means = torch.cat([A_mean, B_mean])\n",
        "\n",
        "# Calculate within-class covariance, between-class covariance, and total covariance\n",
        "within_class_cov = torch.stack([A.T.cov(correction=0), B.T.cov(correction=0)])\n",
        "within_class_cov = (within_class_cov * class_weights[:,None,None]).sum(dim=0)\n",
        "between_class_cov = class_means.T.cov(correction=0)\n",
        "total_cov = C.T.cov(correction=0)\n",
        "\n",
        "# Show that the sum of within-class covariance + between-class covariance = total covariance\n",
        "print('Within-Class + Between-Class = Total Covariance')\n",
        "print(torch.allclose(within_class_cov + between_class_cov, total_cov))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cku2jaxHa4Ys"
      },
      "source": [
        "In their paper, rather than report the entire within-class covariance matrix, Papyan et al. (2020) show the trace of the within-class covariance matrix multiplied by the Penrose-Moore pseudoinverse of the between-class covariance, divided by the number of classes, i.e.\n",
        "\n",
        "$Tr(\\sum_{W}\\sum_{B}^{\\dagger})/C$\n",
        "\n",
        "They define this as the inverse signal-to-noise ratio for classification problems, and explain that it scales and rotates the within-class covariance (noise) by the pseudoinverse of the between-class covariance (signal).\n",
        "\n",
        "However, I have a hard time relating this metric to either of the more straightforward metrics $Tr(\\sum_{W})$ or $Tr(\\sum_{B})$, and sometimes it is close to zero even while $Tr(\\sum_{W}) > Tr(\\sum_{B})$, so I am uncertain of its use to prove that within-class covariance goes to zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWXGxeDOmP0L",
        "outputId": "f4b69d6b-bc00-439a-f256-faecf06d1f0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Within-class:    0.25\n",
            "Between-class:   0.08\n",
            "Noise-to-Signal: 0.06\n"
          ]
        }
      ],
      "source": [
        "# Inverse Signal-to-Noise Ratio\n",
        "\n",
        "# C = m x n data matrix\n",
        "C = torch.rand((6,10))\n",
        "\n",
        "# Split the dataset C into unequal classes A and B\n",
        "A = C[:2]\n",
        "B = C[2:]\n",
        "\n",
        "# Balance the within-class covariances by weighting each class' covariance by the number of samples in that class\n",
        "class_weights = torch.tensor([A.shape[0], B.shape[0]])\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "\n",
        "# Balance the between-class covariance by repeating each class' mean by the number of samples in that class\n",
        "C_mean = C.mean(dim=0, keepdim=True)\n",
        "A_mean = A.mean(dim=0, keepdim=True).repeat((A.shape[0],1))\n",
        "B_mean = B.mean(dim=0, keepdim=True).repeat((B.shape[0],1))\n",
        "class_means = torch.cat([A_mean, B_mean])\n",
        "\n",
        "# Calculate within-class covariance, between-class covariance, and noise-to-signal ratio\n",
        "within_class_cov = torch.stack([A.T.cov(correction=0), B.T.cov(correction=0)])\n",
        "within_class_cov = (within_class_cov * class_weights[:,None,None]).sum(dim=0)\n",
        "between_class_cov = class_means.T.cov(correction=0)\n",
        "noise_to_signal = torch.matmul(within_class_cov, torch.linalg.pinv(between_class_cov))\n",
        "\n",
        "# Print the trace of each covariance matrix divided by the number of classes\n",
        "within_class_trace = torch.trace(within_class_cov) / 2\n",
        "between_class_trace = torch.trace(between_class_cov) / 2\n",
        "noise_to_signal_trace = torch.trace(noise_to_signal) / 2\n",
        "\n",
        "print(f'Within-class:    {within_class_trace:.2f}')\n",
        "print(f'Between-class:   {between_class_trace:.2f}')\n",
        "print(f'Noise-to-Signal: {noise_to_signal_trace:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j6JrfAYsRzl"
      },
      "source": [
        "## NC2: Convergence to Simplex ETF\n",
        "\n",
        "The paper defines a simplex ETF as a collection of points in $R^{c}$ specified by the columns of\n",
        "\n",
        "$M^{\\star} = \\sqrt{\\frac{C}{C-1}}(I-\\frac{1}{c}\\mathbf{1})$\n",
        "\n",
        "where $C$ is the number of classes, $I \\in R^{CxC}$ is the Identity Matrix, and $\\mathbf{1} \\in R^{CxC}$ is a matrix of all ones. They then loosen this condition to allow for other poses and rescaling, so that $M = \\alpha UM^{\\star}$ where $\\alpha \\in R_{+}$ is a scalar and $U \\in R^{PxC}$ is a partial orthogonal matrix s.t. $U^{T}U=I$.\n",
        "\n",
        "NC2 states that, in the terminal phase of training (TPT), the centered class means form a simplex ETF. This is measured by three metrics: 1) convergence of centered class means to the same norm; 2) convergence of centered class means to the same cosine similarity with each other; and 3) convergence of all centered class means to a cosine similarity of -1/(C-1), where C is the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBpHXUqTaTzD",
        "outputId": "f1057985-1cfd-430d-ad5f-714945371a00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norms of centered random class means: ['0.59', '0.66', '0.89']\n",
            "Norms of centered orthonormal means:  ['0.82', '0.82', '0.82']\n",
            "\n",
            "Cosine similarity matrix of centered random class means:\n",
            "tensor([[ 1.0000,  0.0254, -0.6793],\n",
            "        [ 0.0254,  1.0000, -0.7509],\n",
            "        [-0.6793, -0.7509,  1.0000]])\n",
            "\n",
            "Cosine similarity matrix of centered orthogonal means:\n",
            "tensor([[ 1.0000, -0.5000, -0.5000],\n",
            "        [-0.5000,  1.0000, -0.5000],\n",
            "        [-0.5000, -0.5000,  1.0000]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We show elsewhere that orthogonal class means with the same norm form an ETF when centered.\n",
        "# We compare three random \"class means\" and three orthogonal \"class means\".\n",
        "rand_means = torch.rand((3,10))\n",
        "ortho_means, _ = torch.linalg.qr(rand_means.T, mode='reduced')\n",
        "ortho_means = ortho_means.T\n",
        "\n",
        "# Center means\n",
        "rand_means = rand_means - rand_means.mean(dim=0, keepdim=True)\n",
        "ortho_means = ortho_means - ortho_means.mean(dim=0, keepdim=True)\n",
        "\n",
        "# Calculate norms\n",
        "rand_norms = torch.linalg.vector_norm(rand_means, dim=1)\n",
        "rand_print = [f'{norm:.2f}' for norm in rand_norms]\n",
        "ortho_norms = torch.linalg.vector_norm(ortho_means, dim=1)\n",
        "ortho_print = [f'{norm:.2f}' for norm in ortho_norms]\n",
        "print('Norms of centered random class means:', rand_print)\n",
        "print('Norms of centered orthonormal means: ', ortho_print)\n",
        "print()\n",
        "\n",
        "# Calculate angles\n",
        "rand_angles = torch.stack([torch.nn.functional.normalize(mean, dim=0) for mean in rand_means])\n",
        "rand_angles = torch.matmul(rand_angles, rand_angles.T)\n",
        "print('Cosine similarity matrix of centered random class means:')\n",
        "print(rand_angles)\n",
        "print()\n",
        "ortho_angles = torch.stack([torch.nn.functional.normalize(mean, dim=0) for mean in ortho_means])\n",
        "ortho_angles = torch.matmul(ortho_angles, ortho_angles.T)\n",
        "print('Cosine similarity matrix of centered orthogonal means:')\n",
        "print(ortho_angles)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4BzCkt720h7"
      },
      "source": [
        "Rather than report all class norms and angles, they report the standard deviation of the norms over their average, the standard deviation of the cosine similarities, and the average difference between the cosine similarity and the max angle of -1/(C-1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s56Jm-zd20Ng",
        "outputId": "e1cc322c-8045-4e72-9348-8a77476e0c03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STD / Avg of random norms: 0.22\n",
            "STD / Avg of orthogonal norms: 0.00\n",
            "\n",
            "STD of random angles: 0.43\n",
            "STD of orthogonal angles: 0.00\n",
            "\n",
            "Avg difference of random angles and max angles: 0.32\n",
            "Avg difference of orthogonal angles and max angles: 0.00\n"
          ]
        }
      ],
      "source": [
        "# Calculate standard deviation / mean of class norms\n",
        "rand_norms_metric = (torch.std(rand_norms) / torch.mean(rand_norms)).item()\n",
        "ortho_norms_metric = (torch.std(ortho_norms) / torch.mean(ortho_norms)).item()\n",
        "print(f'STD / Avg of random norms: {rand_norms_metric:.2f}')\n",
        "print(f'STD / Avg of orthogonal norms: {ortho_norms_metric:.2f}')\n",
        "print()\n",
        "\n",
        "# Calculate standard deviation of interclass angles\n",
        "rows, cols = torch.triu_indices(rand_angles.shape[0], rand_angles.shape[1], offset=1)\n",
        "rand_angles_metric = torch.std(rand_angles[rows, cols]).item()\n",
        "ortho_angles_metric = torch.std(ortho_angles[rows, cols]).item()\n",
        "print(f'STD of random angles: {rand_angles_metric:.2f}')\n",
        "print(f'STD of orthogonal angles: {ortho_angles_metric:.2f}')\n",
        "print()\n",
        "\n",
        "# Calculate average difference of interclass angles and max angles\n",
        "rows, cols = torch.triu_indices(rand_angles.shape[0], rand_angles.shape[1], offset=1)\n",
        "rand_shift_angles = torch.mean(torch.abs(rand_angles[rows,cols]+1/(3-1))).item()\n",
        "ortho_shift_angles = torch.mean(torch.abs(ortho_angles[rows,cols]+1/(3-1))).item()\n",
        "print(f'Avg difference of random angles and max angles: {rand_shift_angles:.2f}')\n",
        "print(f'Avg difference of orthogonal angles and max angles: {ortho_shift_angles:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJXGFzZrl0dz"
      },
      "source": [
        "## NC3: Self-Duality\n",
        "\n",
        "Self-duality refers to the phenomenon of the centered class means and the weights of the linear classifier approaching scaled versions of each other. To measure this, they take the difference between the class mean matrix and the linear weight matrix, both normalized by their Frobenius norms, and take the norm of that difference matrix, which should go to zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsqdaX2ex0a_",
        "outputId": "1c54aeb5-a9b0-4d20-b86f-ac2e783b1e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Self-Dual Matrix Norm: 0.00\n",
            "Non-Dual Matrix Norm:  1.27\n"
          ]
        }
      ],
      "source": [
        "# Calculate the difference between class means and linear weights\n",
        "\n",
        "# Create \"class means\" and corresponding \"dual weights\" (i.e. a scaled version of the means),\n",
        "# as well as a separate \"random weights\" to compare. We make all of these orthogonal to fit NC2.\n",
        "class_means = torch.rand((3,10))\n",
        "class_means = torch.linalg.qr(class_means.T, mode='reduced')[0].T\n",
        "class_means = class_means - class_means.mean(dim=0, keepdim=True)\n",
        "dual_weights = class_means * torch.rand((1))\n",
        "rand_weights = torch.rand((3,10))\n",
        "rand_weights = torch.linalg.qr(rand_weights.T, mode='reduced')[0].T\n",
        "\n",
        "# Normalize matrices by their Frobenius norms\n",
        "class_means = class_means / torch.linalg.matrix_norm(class_means, ord='fro')\n",
        "dual_weights = dual_weights / torch.linalg.matrix_norm(dual_weights, ord='fro')\n",
        "rand_weights = rand_weights / torch.linalg.matrix_norm(rand_weights, ord='fro')\n",
        "\n",
        "# Calculate difference between the class means and dual weights, and between the class means and random weights\n",
        "self_dual = class_means - dual_weights\n",
        "non_dual = class_means - rand_weights\n",
        "\n",
        "# Calculate the Frobenius norm of both difference matrices\n",
        "print(f'Self-Dual Matrix Norm: {torch.linalg.matrix_norm(self_dual).item():.2f}')\n",
        "print(f'Non-Dual Matrix Norm:  {torch.linalg.matrix_norm(non_dual).item():.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOHzpQQZLMPb"
      },
      "source": [
        "## NC4: Nearest Class Center\n",
        "\n",
        "In the terminal phase of training (TPT), given a specific input, the output of the model becomes equivalent to selecting the class mean most closely aligned with that input's final-layer activations. This follows from NC1-NC3, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fHCLYTMObwu",
        "outputId": "fdd581eb-5312-43ad-9d6c-10b29fdb86c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear outputs:      tensor([5, 2, 2, 1, 5, 5, 2, 2])\n",
            "Nearest class means: tensor([5, 2, 2, 1, 5, 5, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "# Following NC1 and NC2, we assume that the features for each class have converged to\n",
        "# their class means, and the centered class means form an ETF.\n",
        "class_means = torch.rand((6,10))\n",
        "class_means = torch.linalg.qr(class_means.T, mode='reduced')[0].T\n",
        "class_means = class_means - class_means.mean(dim=0, keepdim=True)\n",
        "\n",
        "# Following NC3, we assume the linear classifier is a scaled version of the class means\n",
        "linear_weights = class_means * torch.rand((1))\n",
        "\n",
        "# Given a new set of input features, we show that max linear output and the max class\n",
        "# mean similarity (as measured by min L2 distance) agree\n",
        "input_features = torch.rand(8,10)\n",
        "linear_outputs = torch.argmax(torch.matmul(linear_weights, input_features.T), dim=0)\n",
        "nearest_class = input_features[None,:,:] - class_means[:,None,:]\n",
        "nearest_class = torch.argmin(torch.linalg.vector_norm(nearest_class, dim=2),dim=0)\n",
        "print('Linear outputs:     ', linear_outputs)\n",
        "print('Nearest class means:', nearest_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKA9GX_AZbcS"
      },
      "source": [
        "One additional note is that the bias vector of the linear classifier (left out / set to zero in this example) could potentially cause disagreement between the linear outputs and nearest class means.\n",
        "\n",
        "They measure this as the percentage of disagreements between the linear output and the nearest class center (NCC) classifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7wMLsyvZpmu",
        "outputId": "33c41bba-bc9f-4df7-ab04-e6ad1806532b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Disagreement between linear output and NCC: 0.0%\n"
          ]
        }
      ],
      "source": [
        "samples = 100\n",
        "input_features = torch.rand(samples,10)\n",
        "linear_outputs = torch.argmax(torch.matmul(linear_weights, input_features.T), dim=0)\n",
        "nearest_class = input_features[None,:,:] - class_means[:,None,:]\n",
        "nearest_class = torch.argmin(torch.linalg.vector_norm(nearest_class, dim=2),dim=0)\n",
        "disagreement = (linear_outputs != nearest_class).sum() / samples\n",
        "print(f'Disagreement between linear output and NCC: {disagreement:.1%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References\n",
        "\n",
        "Papyan, V., Han, X. Y., & Donoho, D. L. Prevalence of Neural Collapse during the Terminal Phase of Deep Learning Training. PNAS, 2020.\n",
        "\n",
        "Hong, W., & Ling, S. Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data. JMLR, 2024. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
